---
layout: default
---

### Automated Web-Scraping for Real Estate Data with AWS, Git & Python
Craigslist offers a cornucupia of data on all sorts of markets, one of richest of which is real estate (of both the residential and commercial variety). That data, if tracked effectively over time, could provide valuable insights about market rate, average sale cycle, or rate of increase in valuation of commercial real estate in particular neighborhoods and cities. In this project, I extend an existing python library (craigslist_scraper) to capture the most relevant data in a given commercial real estate post, and write it to an AWS cloud database (RDS). Using a crontab on an AWS S3 server, I can automate the scraper to run every day on new posts.

### Extending Python's craigslist_scraper library
I found this library on stackoverflow, and it is simple and functional. Calling craigslist_scraper.scraper(url) on a url returns an object with methods corresponding to various features of the post. The library is built on top of BeautifulSoup4, and object returned by scrape_url contains a .soup object which I used to extend the library. 

> In [4]: sc_object= scraper.scrape_url(urls[0])
> 
> In [5]: sc_object.<tab>
> sc_object.attrs         sc_object.parse_attr    sc_object.parse_string  sc_object.soup
> sc_object.get_text      sc_object.parse_int     sc_object.price         sc_object.title```

As you can see below, while title and price are methods of the object, a lot of data that would be specifically valuable for real estate analysis is missed. Specifically, there are patterns in the construction of the webpages which would allow us to identify address, longitude, latitude, total sqfeet, the full title of the posting, and the neighborhood of the posting. all of the data is valuable for our purposes. My AddMeta function uses various bs4 methods to parse the soup and return that data.

```python
def AddMeta(x):
	if x.soup:
		
		try:
			x.address = x.soup.findAll(attrs={'class': 'mapaddress'})[0].text
		except:
			x.address = None	
		try:	
			x.latitude = float(re_data.soup.findAll(attrs={'class': 'mapbox'})[0].findChildren()[0].get('data-latitude'))
		except:
			x.latitude = None	
		try:	
			x.longitude = float(re_data.soup.findAll(attrs={'class': 'mapbox'})[0].findChildren()[0].get('data-longitude'))
		except:
			x.longitude = None
		try:	
			x.sqfeet = int(x.soup.findAll(attrs={'class': 'housing'})[0].text.replace('/','').replace('ft2','').replace('-','').strip())
		except:
			x.sqfeet = None
		try:	
			x.fulltitle = x.soup.findAll(attrs={'class': 'postingtitletext'})[0].text.replace(' \n\n\nhide this posting\n\n\n\n    unhide\n  \n\n','').replace('\n','')
		except:
			x.fulltitle = None
		try:	
			x.neighborhood = x.fulltitle[x.fulltitle.find('(')+1:x.fulltitle.find(')')]
		except:
			x.neighborhood = None
	else:
		pass```

### Cloud databases and automation with AWS

The free tier of AWS allows one to provision a cloud server instance and a database instance (I use postgres by default). The administration of these services isn't horribly complicated, but I'll skip that part for this posting. Once a linux server is provisioned and I have ssh'ed in for the first time, the following commands provision a virtual environment, install git, and clone my repository onto the server:

```shell
virtualenv venv
source venv/bin/activate
sudo yum install git
ssh-keygen
cat /home/ec2-user/.ssh/id_rsa.pub
git clone git@github.com:willpettengill/real-estate-craig.git ```

The key generated by ```ssh-keygen``` should be added to one's user profile on git. Now, open a crontab with ```crontab -e```, and schedule a job to run the python script daily at 2 am with ```0 2 * * * python /real-estate-craig/cl_scraper.py``` and watch in amazement as the script populates the database over the course of the day (scrape slowly so that craiglist doesn't ban your IP!)


Next Steps:

Make time posting and time down parameters in the database table. So you can use shortest time on the market for the best analysis. Then you give confidence intervals on your graphs to show which variables are uncertain. Then obviously greater density of samples gives greater confidence. but trigger algorithm to recognize where there are real increases in value on good sample sizes. 